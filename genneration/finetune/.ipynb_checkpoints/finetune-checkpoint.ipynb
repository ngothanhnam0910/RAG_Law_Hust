{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datetime import datetime\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from trl import setup_chat_format\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "base_model_id = \"/home/namnt/md1/NAMNT_DA2/base_llm/T-VisStar-7B-v0.1\"\n",
    "output_dir = \"/mnt/md1/mlflow/DATN/26_09_2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# define function for convert dataset to type of conversation\n",
    "system_message = \"\"\"Là một chuyên gia đọc hiểu, hãy trả lời question dưới đây dựa vào context mà tôi cung cấp.\n",
    "Câu trả lời ngắn gọn, chính xác. Nếu câu nào không có câu trả lời thì hãy trả về không tìm thấy thông tin.\n",
    "Dưới đây là thông tin của context: {context}\n",
    "\"\"\"\n",
    " \n",
    "def create_conversation(row):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=row[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": row[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": row[\"answer\"]}\n",
    "    ]\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# apply create_conversation\n",
    "\n",
    "train_csv = '/point/namnt/DATN/genneration/data/qa_output.csv'\n",
    "test_csv = '/point/namnt/DATN/genneration/data/data_test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Apply the prompt template to the train dataset\n",
    "train_dataset = train_dataset.map(create_conversation)\n",
    "test_dataset = test_dataset.map(create_conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# init tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=self.max_length,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id, \n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing for efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=16,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \n",
    "        \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    ")\n",
    "# Load peft_config\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    report_to=\"tensorboard\",\n",
    "    run_name= f\"Vistral-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n",
    "    output_dir= output_dir,\n",
    "    do_train = True,\n",
    "    do_eval= True,\n",
    "    per_device_train_batch_size= 1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    bf16=True,\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    eval_strategy = \"steps\",\n",
    "    save_strategy = \"steps\",\n",
    "    max_steps=8000,\n",
    "    save_steps=250,\n",
    "    logging_steps=250,\n",
    "    eval_steps=250,\n",
    "    warmup_steps=250,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    )\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset= train_dataset, \n",
    "    eval_dataset = test_dataset,\n",
    "    max_seq_length= 3072,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
